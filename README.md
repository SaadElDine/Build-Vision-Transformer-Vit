# Build-Vision-Transformer-Vit

Vision transformers (ViTs) are another class of neural networks which have emerged as a game-changer in computer vision, surpassing traditional CNN models in various tasks. By leveraging self-attention, Vision Transformers excel at capturing global dependencies within images, allowing them to understand context and extract meaningful information. This newfound capability has led to improved performance in tasks like image classification, object detection, and image segmentation.

![image](https://github.com/SaadElDine/Build-Vision-Transformer-Vit/assets/113860522/48760809-d67e-4e7e-9f33-78af270f426e)

# Sequence of steps:
1. Split an image into patches. ===== Tokenization
2. Flatten the patches.
3. Produce lower-dimensional linear embeddings from the flattened patches.
4. Add positional embeddings.
5. Feed the sequence as an input to a standard transformer encoder.
